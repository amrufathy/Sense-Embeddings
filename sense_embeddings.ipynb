{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sense_embeddings.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "ytdLOhS_pAV6",
        "aT9kdhnVuyoT",
        "iZ0NUFTFu4T1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOxvwd6vp0rx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import multiprocessing\n",
        "\n",
        "from lxml.etree import iterparse, tostring"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytdLOhS_pAV6",
        "colab_type": "text"
      },
      "source": [
        "## Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFXtumop2Bji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUxYJoxvo-VS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download datasets\n",
        "# !wget 'http://lcl.uniroma1.it/eurosense/data/eurosense.v1.0.high-precision.tar.gz'\n",
        "# !wget 'http://lcl.uniroma1.it/sew/data/sew_conservative.tar.gz'\n",
        "# !wget 'http://trainomatic.org/data/train-o-matic-data.zip'\n",
        "\n",
        "# !cp 'eurosense.v1.0.high-precision.tar.gz' '/content/gdrive/My Drive/eurosense.v1.0.high-precision.tar.gz' \n",
        "# !cp 'sew_conservative.tar.gz' '/content/gdrive/My Drive/sew_conservative.tar.gz' \n",
        "# !cp 'train-o-matic-data.zip' '/content/gdrive/My Drive/train-o-matic-data.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM2AGW7kecpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy file from google drive to local\n",
        "# !cp '/content/gdrive/My Drive/eurosense.v1.0.high-precision.tar.gz' 'eurosense.v1.0.high-precision.tar.gz'\n",
        "!cp '/content/gdrive/My Drive/sew_conservative.tar.gz' 'sew_conservative.tar.gz'\n",
        "# !cp '/content/gdrive/My Drive/train-o-matic-data.zip' 'train-o-matic-data.zip'\n",
        "\n",
        "# Unzip datasets\n",
        "# !tar -xvf '/content/gdrive/My Drive/eurosense.v1.0.high-precision.tar.gz'\n",
        "!tar -xf 'sew_conservative.tar.gz'\n",
        "# !unzip -q 'train-o-matic-data.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNnLnNDGp6pC",
        "colab_type": "text"
      },
      "source": [
        "## Parse Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N6FP5hQTah3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_bn2wn_mapping(path):\n",
        "  \"\"\"\n",
        "  Returns a dictionary with a mapping between\n",
        "    BabelNet synsets and WordNet synsets\n",
        "  \"\"\"\n",
        "  bn2wn = dict()\n",
        "\n",
        "  with open(path) as f:\n",
        "    for line in f:\n",
        "      # TODO: check the line with 3 entries\n",
        "      bn, wn = line.strip().split()[:2]\n",
        "      bn2wn[bn] = wn\n",
        "  \n",
        "  return bn2wn\n",
        "\n",
        "bn2wn = get_bn2wn_mapping('/content/gdrive/My Drive/bn2wn_mapping.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sviFACrXwsgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def process_text(s):\n",
        "  \"\"\"\n",
        "  Removes punctuation and multiple consecutive\n",
        "    spaces from text\n",
        "  \"\"\"\n",
        "  # remove punctuation characters\n",
        "  s = s.translate(\n",
        "     str.maketrans('', '', string.punctuation))\n",
        "  # remove multiple consecutive spaces\n",
        "  s = re.sub(' +', ' ', s)\n",
        "  \n",
        "  return s.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT9kdhnVuyoT",
        "colab_type": "text"
      },
      "source": [
        "### Parse Eurosense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mJF8QQJiunJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_longest_lemma_from_anchor(lemm_anchor, lemmas):\n",
        "  \"\"\"\n",
        "  Returns the longest lemma containing the `anchor`\n",
        "    string. According to high precision specification of Eurosense.\n",
        "  \"\"\"\n",
        "  relevant_lemmas = list(filter(lambda x: lemm_anchor in x, lemmas))\n",
        "  longest_lemma = max(relevant_lemmas, key=len)\n",
        "  \n",
        "  return longest_lemma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXHFZuhCp-7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_eurosense(xml_path):\n",
        "  sentences = []\n",
        "  \n",
        "  context = iterparse(xml_path, events=('start', 'end'))\n",
        "  for idx, (_, element) in enumerate(context):\n",
        "    # if valid sentence\n",
        "    if element.tag == 'sentence' and 'id' in element.attrib:\n",
        "      # get english text\n",
        "      eng = element.xpath('text[@lang=\"en\"]')\n",
        "      \n",
        "      if not eng or not eng[0].text: continue\n",
        "      \n",
        "      sentence = process_text(eng[0].text)\n",
        "\n",
        "      # get english annotations\n",
        "      annotations = element.xpath('annotations/annotation[@lang=\"en\"]')\n",
        "      anchor2lemma, lemma2synset = dict(), dict()\n",
        "\n",
        "      # extract lemma_synset pairs\n",
        "      for child in annotations:\n",
        "        bn = child.text\n",
        "        anchor = process_text(child.get('anchor').lower())\n",
        "\n",
        "        if bn in bn2wn and anchor:\n",
        "          lemma = '_'.join(child.get('lemma').split()).lower()\n",
        "          anchor2lemma[anchor] = lemma\n",
        "          lemma2synset[lemma] = bn2wn[bn]\n",
        "\n",
        "      # replace annotated anchors with lemma_synset pair\n",
        "      sorted_anchors = sorted(anchor2lemma.keys(), key=len, reverse=True)\n",
        "      for i, anchor in enumerate(sorted_anchors):\n",
        "        # check if current anchor was contained in a bigger anchor before\n",
        "        if anchor not in ' '.join(sorted_anchors[:i]):\n",
        "          lemm_anchor = anchor2lemma[anchor]\n",
        "          longest_lemma = get_longest_lemma_from_anchor(lemm_anchor, anchor2lemma.values())\n",
        "          synset = lemma2synset[longest_lemma]\n",
        "\n",
        "          old = r'\\b{}\\b'.format(anchor)\n",
        "          new = '{}_{}'.format(longest_lemma, synset)\n",
        "          sentence = re.sub(old, new, sentence)\n",
        "      \n",
        "      sentences.append(sentence.lower().split())\n",
        "      \n",
        "    element.clear()\n",
        "  \n",
        "  return sentences\n",
        "\n",
        "xml_path = 'EuroSense/eurosense.v1.0.high-precision.xml'\n",
        "tik = time.time()\n",
        "eurosense_sents = parse_eurosense(xml_path)\n",
        "tok = time.time()\n",
        "print('Parsing eurosense: {} minutes'.format((tok - tik) / 60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y7DNpam3QYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('eurosense_parsed_sentences.txt', 'w') as f:\n",
        "  for sent in eurosense_sents:\n",
        "    str_sent = ' '.join(sent)\n",
        "    f.write(str_sent + '\\n')\n",
        "    \n",
        "!cp 'eurosense_parsed_sentences.txt' '/content/gdrive/My Drive/eurosense_parsed_sentences.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kelq2LHUJsy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/gdrive/My Drive/eurosense_parsed_sentences.txt' 'eurosense_parsed_sentences.txt'\n",
        "\n",
        "eurosense_sents = []\n",
        "with open('eurosense_parsed_sentences.txt') as f:\n",
        "  for line in f:\n",
        "    eurosense_sents.append(line.strip().split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ0NUFTFu4T1",
        "colab_type": "text"
      },
      "source": [
        "### Parse SEW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIGsmFRvcqvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/gdrive/My Drive/sample.xml' 'sample.xml'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuUs50U2u812",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glob import iglob\n",
        "\n",
        "def parse_sew():\n",
        "  sentences = []\n",
        "\n",
        "  for i, xml in enumerate(iglob('sew_conservative/*/*.xml')):\n",
        "    # extract first 4M sentences\n",
        "    if len(sentences) > 4_000_000:\n",
        "      print(i)\n",
        "      break\n",
        "\n",
        "    context = iterparse(xml, events=('start', 'end'))\n",
        "    for idx, (_, element) in enumerate(context):\n",
        "\n",
        "      if element.tag.lower() == 'wikiarticle':\n",
        "        # article text\n",
        "        article_text = process_text(element.xpath('text')[0].text)\n",
        "\n",
        "        # all annotations\n",
        "        annotations = element.xpath('annotations/annotation')\n",
        "        for child in annotations:\n",
        "          bn = child.xpath('babelNetID')\n",
        "          mention = child.xpath('mention')\n",
        "          if not bn or not mention or not mention[0].text:\n",
        "            continue\n",
        "\n",
        "          bn = bn[0].text\n",
        "          if bn in bn2wn:\n",
        "            anchor = process_text(mention[0].text)\n",
        "            new_anchor = '_'.join(anchor.split())\n",
        "\n",
        "            # this replacement technique works 90% of the time\n",
        "            old = r'\\b{}\\b'.format(anchor)\n",
        "            new = '{}_{}'.format(new_anchor, bn2wn[bn])\n",
        "            article_text = re.sub(old, new, article_text, count=1)\n",
        "            \n",
        "        # randomly pick 20% of article sentences\n",
        "        article_sents = article_text.split('\\n')\n",
        "        selected_sents = random.sample(article_sents, int(0.2 * len(article_sents)))\n",
        "        \n",
        "        for s in selected_sents:\n",
        "          sentences.append(s.split())\n",
        "\n",
        "        # only need one article element\n",
        "        element.clear()\n",
        "        break\n",
        "\n",
        "  return sentences\n",
        "\n",
        "tik = time.time()\n",
        "sew_sents = parse_sew()\n",
        "tok = time.time()\n",
        "print('Parsing SEW: {} minutes'.format((tok - tik) / 60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D9b9-3ws4y_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('sew_parsed_sentences', 'w') as f:\n",
        "  for sent in sew_sents:\n",
        "    str_sent = ' '.join(sent)\n",
        "    f.write(str_sent + '\\n')\n",
        "    \n",
        "!cp 'sew_parsed_sentences.txt' '/content/gdrive/My Drive/sew_parsed_sentences.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgRPsd3UMppJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/gdrive/My Drive/sew_parsed_sentences.txt' 'sew_parsed_sentences.txt'\n",
        "\n",
        "sew_sents = []\n",
        "with open('sew_parsed_sentences.txt') as f:\n",
        "  for line in f:\n",
        "    sew_sents.append(line.strip().split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj3m9fxYrvyL",
        "colab_type": "text"
      },
      "source": [
        "### Parse Train-O-Matic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZGUPB3Pr0mA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glob import iglob\n",
        "\n",
        "def parse_trainomatic(path):\n",
        "  sentences = []\n",
        "\n",
        "  for i, xml in enumerate(iglob(path)):\n",
        "    \n",
        "    context = iterparse(xml, events=('start', 'end'))\n",
        "    for idx, (_, element) in enumerate(context):\n",
        "      \n",
        "      if element.tag.lower() == 'corpus':\n",
        "        child = element.xpath('lexelt')\n",
        "        \n",
        "        if not child: continue\n",
        "\n",
        "        lemma = child[0].get('item').split('.')[0]\n",
        "        instances = child[0].xpath('instance')\n",
        "        \n",
        "        for ins in instances:\n",
        "          answer, context = ins.xpath('answer'), ins.xpath('context')\n",
        "          \n",
        "          if answer and context:\n",
        "            wn = ins.xpath('answer/@senseId')[0].split(':')[1]\n",
        "            pair = '{}_{}'.format(lemma, wn)\n",
        "\n",
        "            sentence = tostring(context[0], method='text', encoding=str).lower()\n",
        "            sentence = sentence.replace(lemma, pair)\n",
        "\n",
        "            sentences.append(sentence.split())\n",
        "\n",
        "      # only one corpus\n",
        "      element.clear()\n",
        "      break\n",
        "      \n",
        "  return sentences\n",
        "\n",
        "      \n",
        "trainomatic_sents = parse_trainomatic('TRAIN-O-MATIC-DATA/EN/EN.500-2.0/*.xml')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LQA8EksKd4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('trainomatic_parsed_sentences.txt', 'w') as f:\n",
        "  for sent in trainomatic_sents:\n",
        "    str_sent = ' '.join(sent)\n",
        "    f.write(str_sent + '\\n')\n",
        "    \n",
        "!cp 'trainomatic_parsed_sentences.txt' '/content/gdrive/My Drive/trainomatic_parsed_sentences.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsMX1UdWKgbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/gdrive/My Drive/trainomatic_parsed_sentences.txt' 'trainomatic_parsed_sentences.txt'\n",
        "\n",
        "trainomatic_sents = []\n",
        "with open('trainomatic_parsed_sentences.txt') as f:\n",
        "  for line in f:\n",
        "    trainomatic_sents.append(line.strip().split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74RB4ZrwuPYq",
        "colab_type": "text"
      },
      "source": [
        "## Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0y90j1Qs69g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_sense_embeddings(path):\n",
        "  \"\"\"\n",
        "  Removes word embeddings from a word2vec\n",
        "    formatted embeddings file\n",
        "  \"\"\"\n",
        "  senses = []\n",
        "  with open(path, 'r') as f:\n",
        "      for line in f:\n",
        "          key = line.split(' ', 1)[0]\n",
        "          if '_' in key:\n",
        "              senses.append(line)\n",
        "\n",
        "  with open(path, 'w') as f:\n",
        "      f.write(\"{} {}\\n\".format(len(senses), len(senses[0].split(' ')) - 1))\n",
        "      for sense in senses:\n",
        "          file.write(sense + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya-53M2TuSFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "train_sents = eurosense_sents + sew_sents + trainomatic_sents\n",
        "\n",
        "model = Word2Vec(size=400, window=10, sample=10e-5, \n",
        "         workers=multiprocessing.cpu_count(), hs=1, negative=0, \n",
        "         iter=15, compute_loss=True)\n",
        "model.build_vocab(train_sents)\n",
        "\n",
        "model.train(train_sents, total_examples=model.corpus_count,\n",
        "        epochs=model.iter, compute_loss=model.compute_loss)\n",
        "model.wv.save_word2vec_format('embeddings.vec', binary=False)\n",
        "filter_sense_embeddings('embeddings.vec')\n",
        "\n",
        "model = model.wv\n",
        "\n",
        "!cp 'embeddings.vec' '/content/gdrive/My Drive/embeddings.vec'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqGjjzjFF3yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "!cp '/content/gdrive/My Drive/embedding_0.22_eurosense_0.2sew.vec' 'embeddings.vec'\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format('embeddings.vec', binary=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_XY1RKbYbiX",
        "colab_type": "text"
      },
      "source": [
        "## Word Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLIArxBzYweB",
        "colab_type": "text"
      },
      "source": [
        "### Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU6qJ_a6Yjo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget 'http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/wordsim353.zip'\n",
        "\n",
        "!unzip -q 'wordsim353.zip' -d 'wordsim353'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4W9e60nvFZl",
        "colab_type": "text"
      },
      "source": [
        "### Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4qY4ZJ3oNAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "from scipy.stats import spearmanr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7uSs6Kwy8A6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def similarity_measure(s1, S1, s2, S2):\n",
        "  return cosine_similarity(s1, s2)\n",
        "  # return weighted_cosine_similarity(s1, S1, s2, S2)\n",
        "  \n",
        "def cosine_similarity(s1, s2):\n",
        "  v1 = model.get_vector(s1)\n",
        "  v2 = model.get_vector(s2)\n",
        "  \n",
        "  return 1 - cosine(v1, v2)\n",
        "\n",
        "def d(s, S):\n",
        "  return model.vocab[s].count / sum([model.vocab[_s].count for _s in S])\n",
        "\n",
        "def weighted_cosine_similarity(s1, S1, s2, S2):\n",
        "  return d(s1, S1) * d(s2, S2) * (cosine_similarity(s1, s2) ** 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lMBC6yz0vKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_associated_sense_embeddings(w):\n",
        "  S = []\n",
        "  for v in model.vocab:\n",
        "    t = v.split('_')\n",
        "    l = ' '.join(t[:-1])\n",
        "    if w == l and len(t) > 1:\n",
        "      S.append(v)\n",
        "      \n",
        "  return S"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzEaUfMYZMtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = 'wordsim353/combined.tab'\n",
        "\n",
        "gold_scores, my_scores = [], []\n",
        "\n",
        "with open(path) as f:\n",
        "  # skip header\n",
        "  next(f)\n",
        "  \n",
        "  for line in f:\n",
        "    w1, w2, sim = line.lower().strip().split('\\t')\n",
        "    \n",
        "    S1 = get_associated_sense_embeddings(w1)\n",
        "    S2 = get_associated_sense_embeddings(w2)\n",
        "    \n",
        "    score = -1.0\n",
        "    \n",
        "    for s1 in S1:\n",
        "      for s2 in S2:\n",
        "        score = max(score, similarity_measure(s1, S1, s2, S2))\n",
        "      \n",
        "    my_scores.append(score)\n",
        "    gold_scores.append(float(sim))\n",
        "    \n",
        "    \n",
        "r = spearmanr(gold_scores, my_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3F43rY8JAmN",
        "colab_type": "text"
      },
      "source": [
        "# Visualize Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xkAFVO7JA51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev2pAyVnW9HR",
        "colab_type": "text"
      },
      "source": [
        "### PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyTG_u27JBhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w1, w2 = 'love', 'hate'\n",
        "n1, n2 = get_associated_sense_embeddings(w1), get_associated_sense_embeddings(w2)\n",
        "all_words = n1\n",
        "all_words.extend(n2)\n",
        "\n",
        "\n",
        "def visualize_pca(words):\n",
        "    V = model[words]\n",
        "    pca = PCA(n_components=2)\n",
        "    result = pca.fit_transform(V)\n",
        "    plt.scatter(result[:, 0], result[:, 1])\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "visualize_pca(all_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZwyRmZb_JTb",
        "colab_type": "text"
      },
      "source": [
        "### t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYU-NzJRG22m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Source: https://towardsdatascience.com/google-news-and-leo-tolstoy-visualizing-word2vec-word-embeddings-with-t-sne-11558d8bd4d\n",
        "def get_clusters(all_words):\n",
        "  embedding_clusters = []\n",
        "  word_clusters = []\n",
        "  for word in all_words:\n",
        "    embeddings = []\n",
        "    words = []\n",
        "    for similar_word, _ in model.most_similar(word, topn=15):\n",
        "        words.append(similar_word)\n",
        "        embeddings.append(model[similar_word])\n",
        "    embedding_clusters.append(embeddings)\n",
        "    word_clusters.append(words)\n",
        "\n",
        "  embedding_clusters = np.array(embedding_clusters)\n",
        "  n, m, k = embedding_clusters.shape\n",
        "  tsne_model_en_2d = TSNE(\n",
        "      perplexity=15, n_components=2, init='pca',\n",
        "      n_iter=3500, random_state=0)\n",
        "  embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(\n",
        "      embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
        "  return embeddings_en_2d, word_clusters\n",
        "\n",
        "def tsne_plot_similar_words(labels, embedding_clusters, word_clusters, filename=None):\n",
        "  plt.figure(figsize=(16, 9))\n",
        "  colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
        "  for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
        "    x = embeddings[:, 0]\n",
        "    y = embeddings[:, 1]\n",
        "    plt.scatter(x, y, c=color, alpha=1.0, label=label)\n",
        "    for i, word in enumerate(words):\n",
        "      plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
        "                       textcoords='offset points', ha='right', va='bottom', size=8)\n",
        "  plt.legend(loc=4)\n",
        "  plt.grid(True)\n",
        "  if filename:\n",
        "      plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "all_words = ['seek_01315613v', 'make_up_02520730v',\n",
        "  'queen_10499355n', 'function_01095218v', 'liner_03673027n']\n",
        "embeddings_en_2d, word_clusters = get_clusters(all_words)\n",
        "tsne_plot_similar_words(all_words, embeddings_en_2d, word_clusters,\n",
        "                          'similar_words.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHogtmkg_L3P",
        "colab_type": "text"
      },
      "source": [
        "### kNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1vnLutMcWXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard_similarity(v1, v2):\n",
        "    intersection = np.dot(v1, v2)\n",
        "    union = (np.linalg.norm(v1) * 2 +\n",
        "                   np.linalg.norm(v2) * 2 - intersection)\n",
        "    return np.round(intersection / union, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjuC5PVYdPxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w1 = 'bank_09213565n' # river bank 09213565n\n",
        "w2 = 'bank_08420278n' # financial inst \"08420278n\"\n",
        "\n",
        "cw = w2\n",
        "v1 = model.get_vector(cw)\n",
        "for sw in model.similar_by_word(cw, topn=10):\n",
        "  v2 = model.get_vector(sw[0])\n",
        "  print(cw, sw, jaccard_similarity(v1, v2))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}